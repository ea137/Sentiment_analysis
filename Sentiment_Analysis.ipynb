{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "np.set_printoptions(precision = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the following Notebook, We will make a Sentiment Analysis from scratch. \n",
    "###### we will be designing a word2vec and tfidf class that mimic countvectorizer and TfidfTransformer from sklearn.feature_extraction.text.They are not made to serve as a definitive replacement for the two original functions, but rather to help us deepen our understanding of how a countvectorizer and tfidftransformer work.\n",
    "###### Important information about the Dataset (taken from the dataset creator) : \n",
    "    1-no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings.\n",
    "    2-train and test sets contain a disjoint set of movies.\n",
    "    3-In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The First Task is to turn the reviews from a text file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def review_extraction(directory):\n",
    "    '''\n",
    "    extracting reviews from a directory\n",
    "    '''\n",
    "    reviews = []\n",
    "    filenames = os.listdir(directory)\n",
    "    for filename in filenames:\n",
    "        with open(directory + filename,'r') as f:\n",
    "            reviews.append(f.read() )\n",
    "    return reviews\n",
    "\n",
    "positive_dir = './train/pos/'\n",
    "positive_reviews = review_extraction(positive_dir)\n",
    "\n",
    "negative_dir = './train/neg/'\n",
    "negative_reviews = review_extraction(negative_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "reviews0 = pd.DataFrame(np.c_[negative_reviews,np.zeros(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews1 = pd.DataFrame(np.c_[positive_reviews,np.ones(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews_train = reviews1.append(reviews0,ignore_index = True)\n",
    "reviews_train = shuffle(reviews_train) # shuffling them to not keep ones at first and then zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17065</th>\n",
       "      <td>I felt brain dead, I'll tell you. This is the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14262</th>\n",
       "      <td>It's a ghost story. It's a cannibalism story. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>The movie held my interest, mainly because Dia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13567</th>\n",
       "      <td>I own almost every Seagal movie (yes even ones...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>Touching and sad movie. Portrays the trials an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews sentiment\n",
       "17065  I felt brain dead, I'll tell you. This is the ...         0\n",
       "14262  It's a ghost story. It's a cannibalism story. ...         0\n",
       "1369   The movie held my interest, mainly because Dia...         1\n",
       "13567  I own almost every Seagal movie (yes even ones...         0\n",
       "10883  Touching and sad movie. Portrays the trials an...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.head(5)# taking a look at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we will start making a Word2Vec class from scratch that will serve the purpose of this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.doc = []\n",
    "        self.dico = {}\n",
    "        self.text = [] #mostly used for storage of a past self\n",
    "\n",
    "    def vocabulary(self):\n",
    "        '''\n",
    "        arr : list of text\n",
    "        return dictionary of vocabulary \n",
    "        '''\n",
    "        return self.dico\n",
    "    \n",
    "    def purge_alph(self):\n",
    "        '''\n",
    "        remove non alpha characters from doc\n",
    "\n",
    "        '''\n",
    "        new_doc = []\n",
    "        for word in self.doc:\n",
    "            regex = re.compile('[^a-zA-Z]')\n",
    "            new_doc += regex.sub(' ', word).split()\n",
    "        return new_doc\n",
    "    \n",
    "    def update(self,new_doc,arr):\n",
    "        '''\n",
    "        updating values of doc and dico and text\n",
    "        '''\n",
    "        self.doc = sorted(list(set(new_doc)))\n",
    "        self.dico = dict(np.c_[self.doc,np.arange(len(self.doc))])\n",
    "        self.text = self.text + list(arr)\n",
    "        \n",
    "    def to_array(self,arr = None,regex= re.compile('[^a-zA-Z]')):\n",
    "        '''\n",
    "        turning array of text to array of integers from the vocabulary,\n",
    "        if a word is not in the vocabulary, it will return an error (you can remove this assertion)\n",
    "        arr : list of texts, if none we use self.text\n",
    "        \n",
    "        you may notice 3 for loops, but the third one is only used to separate words that are separated \n",
    "        by a special character, so most of the time it only has one iteration.\n",
    "        '''\n",
    "        if type(arr) == type(None):\n",
    "            arr = self.text\n",
    "        final_vector = []\n",
    "        for text in arr:\n",
    "            vector = np.zeros(len(self.doc),dtype = 'int')\n",
    "            for words in text.split(' '):\n",
    "                words = regex.sub(' ', words).split()\n",
    "                for word in words: #special cases when a special character is between two words for example ok.hello \n",
    "                                   # will become ok and hello.\n",
    "                    assert word in self.doc , f'{word} not in Vocabulary'  # feel free to remove this assertion\n",
    "                    try:\n",
    "                        index = int(self.dico[word])\n",
    "                        vector[index] +=1\n",
    "                    except:\n",
    "                        pass\n",
    "            final_vector.append(list(vector))\n",
    "        return final_vector\n",
    "                    \n",
    "        \n",
    "    def fit(self, arr):\n",
    "        '''\n",
    "        fits docs and add new vocabularies\n",
    "        '''\n",
    "        unique = self.doc\n",
    "        for text in arr:\n",
    "            txt = text.split()\n",
    "            unique = unique + list(set( txt ))\n",
    "            \n",
    "        self.doc  = sorted(list(set(unique)))\n",
    "        self.dico = dict(np.c_[self.doc,np.arange(len(self.doc))])\n",
    "        new_doc = self.purge_alph()\n",
    "        self.update(new_doc,arr)\n",
    "        return self\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array(['the sun is shining',\n",
    "                'the weather is sweet',\n",
    "                'the sun is shining, the weather is sweet, and one and one is two'])\n",
    "test= Word2Vec()\n",
    "test.fit(docs)\n",
    "test.vocabulary()\n",
    "test.to_array(docs)\n",
    "\n",
    "'''\n",
    "or \n",
    "\n",
    "'''\n",
    "test= Word2Vec()\n",
    "bag = test.fit(docs)\n",
    "#bag.to_array()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define some norms before designing tf-idf :\n",
    "\n",
    "##### to make a normalization, we choose one norm and divide the elements of the array by its resulting value. the examples given  belows are just one dimensional, but we usually face cases with multidimensional matrices, then you will need to choose what axis to choose for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 : given an array, we are adding up the absolute value of its element, and the resulting number is the l1 norm.\n",
    "\n",
    "---example : $$\\text{L1} ([1,2,3,4]) = |1| + |2| + |3| + |4| = 10$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 : given an array, we are adding up the square of its element, and the square root of the resulting number is        the l2 norm.\n",
    "\n",
    "-- example : $$\\text{L2} ([1,2,3,4])= \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 : given an array, we are taking the maximum value of the absolute values of its elements\n",
    "\n",
    "-- example : $$\\text{maxnorm} ([1,2,3,-4])= max(1|,|2|,|3|,|-4|) = 4$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizations:\n",
    "    ''' \n",
    "    l1/l2/maxnorm normalizations. \n",
    "    Be aware, it doesn't catch zero division. you can improve it to catch zero vectors in the re_shape method helper.\n",
    "    ''' \n",
    "    def __init__(self,matrix):\n",
    "        self.matrix = np.array(matrix)        \n",
    "    def re_shape(self,norm,axis):\n",
    "        '''\n",
    "        reshapes the norm to fit the matrix\n",
    "        '''\n",
    "        if axis ==1:\n",
    "            resultant = np.repeat(norm,self.matrix.shape[axis]).reshape(self.matrix.shape)\n",
    "        elif axis==0:\n",
    "            resultant = np.repeat(norm,self.matrix.shape[axis]).reshape(self.matrix.shape[::-1]).T\n",
    "        return resultant\n",
    "    def l1_normalization(self,axis=1):\n",
    "        '''\n",
    "        returns l1-normalized matrix\n",
    "        '''\n",
    "        l1_values = np.sum(abs(self.matrix),axis)\n",
    "        resultant = self.re_shape(l1_values,axis)\n",
    "        l1 = self.matrix / resultant\n",
    "        return l1\n",
    "    def l2_normalization(self,axis =1):\n",
    "        '''\n",
    "        returns l2-normalized matrix\n",
    "        '''\n",
    "        l2_values = np.sqrt(np.sum(self.matrix**2,axis))\n",
    "        resultant = self.re_shape(l2_values,axis)\n",
    "        l2 = self.matrix / resultant\n",
    "        return l2\n",
    "    def maxnorm_normalization(self,axis =1):\n",
    "        '''\n",
    "        returns maxnorm-normalized matrix\n",
    "        '''\n",
    "        maxnorm_values = abs(self.matrix).max(axis)\n",
    "        resultant = self.re_shape(maxnorm_values,axis)\n",
    "        maxnorm = self.matrix / resultant\n",
    "        return maxnorm\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency and Inverse Document Frequency :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{raw term frequency tf(t,d)---number of time a term t occurs in a document d} $\n",
    "$ \\text{Inverse document frequency idf(t,d)---number of documents d that contains the term t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{ tf(t,d) = } \\frac{n_t}{n_w}  \\text{ ,with :} \\bigg\\{_{n_w \\text{the total number of terms in the document}}^{n_t \\text{count of t appears in a document}} $  \n",
    "$ \\text{idf(t,d) = } log_e(\\frac{n_d}{n_{dt}})+1 \\text{ ,with :} \\bigg\\{^{n_d \\text{the total number of documents}}_{n_{dt} \\text{the number of documents with term t in it}}$\n",
    "$ \\text{We are using the logarithm to ensure that low document frequencies are not given too much weight.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{ tfidf(t,d) = tf(t,d) * idf(t,d) }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf:\n",
    "    def __init__(self,matrix,axis = 1):\n",
    "        self.matrix = np.array(matrix)\n",
    "        self.axis = axis\n",
    "\n",
    "    def _Norm(self,tf,norm):\n",
    "        '''\n",
    "        Normalize the matrix with the chosen norm\n",
    "        '''\n",
    "        if norm == 'l2':\n",
    "            tf = Normalizations(tf).l2_normalization()\n",
    "        elif norm == 'l1':\n",
    "            tf = Normalizations(tf).l1_normalization()\n",
    "        elif norm == 'maxnorm':\n",
    "            tf = Normalizations(tf).maxnorm_normalization()\n",
    "        return tf\n",
    "    def tfidf(self,norm = 'l2',idf = True,smooth = True):\n",
    "        '''\n",
    "        return matrix adjusted with tfidf\n",
    "        '''\n",
    "        tf = self.tf(norm,idf,smooth)\n",
    "        if idf:\n",
    "            tfidf = tf * self.idf(smooth)\n",
    "        else:\n",
    "            tfidf = tf\n",
    "        tfidf = self._Norm(tfidf,norm)\n",
    "        return tfidf\n",
    "    def tf(self,norm = 'l2',idf = True,smooth = True):\n",
    "        '''\n",
    "        returns tf\n",
    "        '''\n",
    "        nw = self.matrix.shape[self.axis] #total number of terms in doc\n",
    "        resultant = np.ones(self.matrix.shape) * nw\n",
    "        tf = self.matrix / nw\n",
    "        tf = self._Norm(tf,norm)\n",
    "        return tf \n",
    "    \n",
    "    def idf(self,smooth):\n",
    "        '''\n",
    "        returns idf\n",
    "        '''\n",
    "        nd = self.matrix.shape[1-self.axis] #total number of documents\n",
    "        ndt = nd - np.sum(self.matrix == 0 ,1-self.axis) # number of documents with term t\n",
    "        idf = np.log((nd)/(ndt)) +1 \n",
    "        if smooth: # prevents zero divisions\n",
    "            idf = np.log((nd+1)/(ndt+1)) +1 # we add on            \n",
    "        if self.axis == 0:\n",
    "            idf= np.repeat(idf,self.matrix.shape[1]).reshape(self.matrix.shape)\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.  , 0.56, 0.56, 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.  , 0.56, 0.43, 0.  , 0.56],\n",
       "       [0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf_idf(bag.to_array(),1)\n",
    "test.tfidf(idf = True,norm = 'l2',smooth = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Let's do some data cleaning : \n",
    "##### preprocessing the reviews to remove html tags,  and moving emoticons to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  hello       haha   :) :p :/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(txt):\n",
    "    '''\n",
    "    light preprocessing of the text\n",
    "    '''\n",
    "    txt+= ' ' # adding space for the emogies not to stick to last word\n",
    "    # removing html tags\n",
    "    txt = re.sub('<[^>]*>',' ', txt) \n",
    "\n",
    "    # puting emoticons to the end and puting everything to lowercase\n",
    "    em_reg = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "    emoticons = re.findall(em_reg,txt)\n",
    "    txt = re.sub(em_reg ,' ', txt.lower() ) + ' '.join(emoticons)\n",
    "    return txt\n",
    "# let's test it\n",
    "preprocessor('<p> hello :) :p :/ haha </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/elarbi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "# to improve the tokenizer, we will also be removing stopwords such as I,me...\n",
    "nltk.download('stopwords') # downloading stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer_porter(text,stop = None):\n",
    "    if type(stop) == type(None):\n",
    "        return [porter.stem(word) for word in text.split()] \n",
    "    else :\n",
    "        return [porter.stem(word) for word in text.split() if word not in stop] \n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words :  ['I', 'know', 'swimmer', 'swim', 'swim', 'pool', 'full', 'swimmer']\n",
      "Not removing stop words :  ['I', 'know', 'a', 'swimmer', 'who', 'is', 'swim', 'in', 'a', 'swim', 'pool', 'full', 'of', 'swimmer']\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "test = 'I know a swimmer who is swimming in a swimming pool full of swimmers'\n",
    "print('Removing stop words : ' , tokenizer_porter(test,stop))\n",
    "print('Not removing stop words : ' , tokenizer_porter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17065</th>\n",
       "      <td>felt brain dead, i'll tell you. worst film eve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14262</th>\n",
       "      <td>ghost story. cannib story. reveng story. poorl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>movi held interest, mainli diann keaton favori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews sentiment\n",
       "17065  felt brain dead, i'll tell you. worst film eve...         0\n",
       "14262  ghost story. cannib story. reveng story. poorl...         0\n",
       "1369   movi held interest, mainli diann keaton favori...         1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the preprocessing on all of the reviews\n",
    "def preprocessing(df,col = 'reviews'):\n",
    "    df[col] = df[col].apply(preprocessor)\n",
    "    df[col] = df[col].apply(lambda x : ' '.join(tokenizer_porter(x,stop)) ) #parsing reviews\n",
    "preprocessing(reviews_train)    \n",
    "reviews_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 out of 25000\n",
      "0.008%\n"
     ]
    }
   ],
   "source": [
    "# let's put everything together\n",
    "# the process below is my understanding of how tfidfVectorizer works\n",
    "word2vec = Word2Vec() # defining the word2vec\n",
    "rev_num = 0\n",
    "rev_tot = reviews_train.shape[0]\n",
    "X_mine = []\n",
    "\n",
    "iteration = 2 # I only use few iterations to compare it with tfidfvectorizer because the computation is \n",
    "#heavy \n",
    "# first we have to preprocess then tokenize the reviews which has already been done above\n",
    "#second we have to add the vocabulary to Word2vec\n",
    "for review in reviews_train.reviews[:iteration]:\n",
    "    rev_num +=1\n",
    "    clear_output(wait = True)\n",
    "    word2vec.fit([review]) # fitting word2vec with the parsed review\n",
    "    print(f'{rev_num} out of {rev_tot}')\n",
    "    print(f'{(rev_num/rev_tot) * 100}%')\n",
    "#third we apply tfidf\n",
    "rev_num = 0\n",
    "for review in reviews_train.reviews[:iteration]:\n",
    "    clear_output(wait = True)\n",
    "    rev_num +=1\n",
    "    tfidf_mine = tf_idf(word2vec.to_array(arr = [review]))\n",
    "    X_mine.append(tfidf_mine.tfidf(idf = True,norm = 'l2',smooth = True) )\n",
    "    print(f'{rev_num} out of {rev_tot}')\n",
    "    print(f'{(rev_num/rev_tot) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents = False, lowercase = False, preprocessor = None,\n",
    "                        tokenizer = tokenizer_porter, use_idf = True, norm = 'l2',\n",
    "                       smooth_idf = True) \n",
    "\n",
    "y = reviews_train.sentiment.values.astype('int')\n",
    "X_lib = tfidf.fit_transform(reviews_train.reviews[:iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary not in my word2vec : ['(in', 'acting.', 'blood;', 'bought.', 'by.', 'campers,', 'cheek,', 'confu', 'dead,', 'dire.', 'dull.', 'each,', 'evolution.', 'granted,', 'gratuitous,', 'here,', \"i'll\", \"i'm\", 'inten', 'it.', 'laughable.', 'minutes.', 'movie,', 'name).', 'place,', 'plague.', 'plot.', 'scenes.', 'seen.', 'story.', 'suppo', 'this.', 'time.', 'tribbiani,', 'violence.', 'wastebasket.', \"what'\", 'wife.', 'wilderness.', 'works.', 'you.']\n",
      "nbr of vocabulary not in my word2vec not taking into account punctuation: ['confu', 'inten', 'suppo']\n"
     ]
    }
   ],
   "source": [
    "# let's compare my Word2vec and the tfidfVectorizer in terms of result (not performance and speed)\n",
    "different = []\n",
    "for voc in tfidf.get_feature_names():\n",
    "    if voc not in word2vec.vocabulary().keys():\n",
    "        different.append(voc)\n",
    "print (f'vocabulary not in my word2vec : {different}') # those words are being removed from my word2vec because I\n",
    "# defined a purge_alph method that gets rid all non-alpha characters in the words and removes the duplicates, \n",
    "#so technically those words are in my word2vec vocabulary, but without the punctuation\n",
    "# Proof :\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "new_different = []\n",
    "for word in different:\n",
    "    new_words = regex.sub(' ', word).split()\n",
    "    for new_word in new_words:\n",
    "        if new_word not in word2vec.vocabulary().keys():\n",
    "            new_different.append(new_word)\n",
    "        \n",
    "print (f'nbr of vocabulary not in my word2vec not taking into account punctuation: { new_different }') \n",
    "# you will usually find weird looking words that are basically that were tokenized differently by Stemmer (due to\n",
    "# the fact that one had a special character and the other one didn't)\n",
    "\n",
    "# the tf-idf resulting matrices will be a bit different because the vocabularies are not the same and the order is\n",
    "# different too, but they both work well, but the speed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will obviously not be making it from scratch everytime I want to use it, but doing once helped me learn a great deal about how some of the features of CountVectorizer, TfidfTransformer, and TfidfVectorizer work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use the TfidfVectorizer for the rest of this Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents = False, lowercase = False, preprocessor = None,# already applied the prepro\n",
    "                        tokenizer = tokenizer_porter, use_idf = True, norm = 'l2', # and lowered the characters\n",
    "                       smooth_idf = True) \n",
    "\n",
    "y = reviews_train.sentiment.values.astype('int')\n",
    "X = tfidf.fit_transform(reviews_train.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "openi = open('tfidf.sav','wb')\n",
    "pickle.dump(tfidf, openi)\n",
    "openi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "dico = tfidf.vocabulary_\n",
    "for i in dico:\n",
    "    dico[i] = int(dico[i]) # converting numpy int to int for json to recognize them\n",
    "with open('vocabulary.json','w') as fp:\n",
    "    json.dump(dico,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.datasets.dump_svmlight_file(X,y,f = 'training_x_y.feat') # saving X and y as libsvm sparse matrixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X,y = sklearn.datasets.load_svmlight_file('training_x_y.feat') # loading them\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 37.5min remaining: 56.2min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 40.3min remaining: 26.9min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 42.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 42.9min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5,\n",
    "                          scoring = 'accuracy',\n",
    "                          random_state = 7,\n",
    "                          n_jobs = -1, # using all the processors \n",
    "                          verbose = 10,\n",
    "                          max_iter = 300)\n",
    "clf.fit(X,y)\n",
    "filename = 'LR_model.sav'\n",
    "check_ = open(filename,'wb')\n",
    "pickle.dump(clf,check_)\n",
    "check_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# loading the saved model\n",
    "filename = 'LR_model.sav'\n",
    "clf = pickle.load(open(filename,'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data\n",
    "positive_dir_test = './test/pos/'\n",
    "positive_reviews = review_extraction(positive_dir_test)\n",
    "\n",
    "negative_dir_test = './test/neg/'\n",
    "negative_reviews_test = review_extraction(negative_dir_test)\n",
    "\n",
    "reviews0_test = pd.DataFrame(np.c_[negative_reviews,np.zeros(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews1_test = pd.DataFrame(np.c_[positive_reviews,np.ones(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews_test = reviews1_test.append(reviews0_test,ignore_index = True)\n",
    "preprocessing(reviews_test)    \n",
    "\n",
    "y_test = reviews_test.sentiment.values.astype('int')\n",
    "X_test = tfidf.transform(reviews_test.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.94 \n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "print(f'accuracy : {round(clf.score(X_test,y_test),2)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good review\n"
     ]
    }
   ],
   "source": [
    "# write whatever comment you want and see whether the machine thinks it's a good or bad comment\n",
    "# write your comment in text_test\n",
    "text_test = \"I really liked the film, especially when the hero saved the girl,\\\n",
    "            for the rest the scenario was banal.\"\n",
    "text_test = preprocessor ( text_test )\n",
    "text_test = tokenizer_porter(text_test,stop)\n",
    "\n",
    "xx_test = tfidf.transform(text_test)\n",
    "prediction = np.mean(clf.predict(xx_test))\n",
    "if prediction <0.5:\n",
    "    print('bad review')\n",
    "elif prediction>.5:\n",
    "    print('good review')\n",
    "else:\n",
    "    print('neutral')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
