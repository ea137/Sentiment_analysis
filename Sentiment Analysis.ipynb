{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "np.set_printoptions(precision = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the following Notebook, We will make a Sentiment Analysis from scratch. \n",
    "###### we will be designing a word2vec and tfidf class that mimic countvectorizer and TfidfTransformer from sklearn.feature_extraction.text.They are not made to serve as a definitive replacement for the two original functions, but rather to help us deepen our understanding of how a countvectorizer and tfidftransformer work.\n",
    "###### Important information about the Dataset (taken from the dataset creator) : \n",
    "    1-no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings.\n",
    "    2-train and test sets contain a disjoint set of movies.\n",
    "    3-In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The First Task is to turn the reviews from a text file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def review_extraction(directory):\n",
    "    '''\n",
    "    extracting reviews from a directory\n",
    "    '''\n",
    "    reviews = []\n",
    "    filenames = os.listdir(directory)\n",
    "    for filename in filenames:\n",
    "        with open(directory + filename,'r') as f:\n",
    "            reviews.append(f.read() )\n",
    "    return reviews\n",
    "\n",
    "positive_dir = './train/pos/'\n",
    "positive_reviews = review_extraction(positive_dir)\n",
    "\n",
    "negative_dir = './train/neg/'\n",
    "negative_reviews = review_extraction(negative_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "reviews0 = pd.DataFrame(np.c_[negative_reviews,np.zeros(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews1 = pd.DataFrame(np.c_[positive_reviews,np.ones(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews_train = reviews1.append(reviews0,ignore_index = True)\n",
    "reviews_train = shuffle(reviews_train) # shuffling them to not keep ones at first and then zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>Another outstanding foreign film which thoroug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>A young cat tries to steal back his brothers s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19387</th>\n",
       "      <td>The movie's storyline is pat and quaint. Two w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17189</th>\n",
       "      <td>Oh, come on people give this film a break. The...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3207</th>\n",
       "      <td>I never bothered to see this movie in theaters...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews sentiment\n",
       "5822   Another outstanding foreign film which thoroug...         1\n",
       "10230  A young cat tries to steal back his brothers s...         1\n",
       "19387  The movie's storyline is pat and quaint. Two w...         0\n",
       "17189  Oh, come on people give this film a break. The...         0\n",
       "3207   I never bothered to see this movie in theaters...         1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.head(5)# taking a look at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we will start making a Word2Vec class from scratch that will serve the purpose of this Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.doc = []\n",
    "        self.dico = {}\n",
    "        self.text = [] #mostly used for storage of a past self\n",
    "\n",
    "    def vocabulary(self):\n",
    "        '''\n",
    "        arr : list of text\n",
    "        return dictionary of vocabulary \n",
    "        '''\n",
    "        return self.dico\n",
    "    \n",
    "    def purge_alph(self):\n",
    "        '''\n",
    "        remove non alpha characters from doc\n",
    "\n",
    "        '''\n",
    "        new_doc = []\n",
    "        for word in self.doc:\n",
    "            regex = re.compile('[^a-zA-Z]')\n",
    "            new_doc += regex.sub(' ', word).split()\n",
    "        return new_doc\n",
    "    \n",
    "    def update(self,new_doc,arr):\n",
    "        '''\n",
    "        updating values of doc and dico and text\n",
    "        '''\n",
    "        self.doc = sorted(list(set(new_doc)))\n",
    "        self.dico = dict(np.c_[self.doc,np.arange(len(self.doc))])\n",
    "        self.text = self.text + list(arr)\n",
    "        \n",
    "    def to_array(self,arr = None,regex= re.compile('[^a-zA-Z]')):\n",
    "        '''\n",
    "        turning array of text to array of integers from the vocabulary,\n",
    "        if a word is not in the vocabulary, it will return an error (you can remove this assertion)\n",
    "        arr : list of texts, if none we use self.text\n",
    "        \n",
    "        you may notice 3 for loops, but the third one is only used to separate words that are separated \n",
    "        by a special character, so most of the time it only has one iteration.\n",
    "        '''\n",
    "        if type(arr) == type(None):\n",
    "            arr = self.text\n",
    "        final_vector = []\n",
    "        for text in arr:\n",
    "            vector = np.zeros(len(self.doc),dtype = 'int')\n",
    "            for words in text.split(' '):\n",
    "                words = regex.sub(' ', words).split()\n",
    "                for word in words: #special cases when a special character is between two words for example ok.hello \n",
    "                                   # will become ok and hello.\n",
    "                    assert word in self.doc , f'{word} not in Vocabulary'  # feel free to remove this assertion\n",
    "                    try:\n",
    "                        index = int(self.dico[word])\n",
    "                        vector[index] +=1\n",
    "                    except:\n",
    "                        pass\n",
    "            final_vector.append(list(vector))\n",
    "        return final_vector\n",
    "                    \n",
    "        \n",
    "    def fit(self, arr):\n",
    "        '''\n",
    "        fits docs and add new vocabularies\n",
    "        '''\n",
    "        unique = self.doc\n",
    "        for text in arr:\n",
    "            txt = text.split()\n",
    "            unique = unique + list(set( txt ))\n",
    "            \n",
    "        self.doc  = sorted(list(set(unique)))\n",
    "        self.dico = dict(np.c_[self.doc,np.arange(len(self.doc))])\n",
    "        new_doc = self.purge_alph()\n",
    "        self.update(new_doc,arr)\n",
    "        return self\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array(['the sun is shining',\n",
    "                'the weather is sweet',\n",
    "                'the sun is shining, the weather is sweet, and one and one is two'])\n",
    "test= Word2Vec()\n",
    "test.fit(docs)\n",
    "test.vocabulary()\n",
    "test.to_array(docs)\n",
    "\n",
    "'''\n",
    "or \n",
    "\n",
    "'''\n",
    "test= Word2Vec()\n",
    "bag = test.fit(docs)\n",
    "#bag.to_array()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's define some norms before designing tf-idf :\n",
    "\n",
    "##### to make a normalization, we choose one norm and divide the elements of the array by its resulting value. the examples given  belows are just one dimensional, but we usually face cases with multidimensional matrices, then you will need to choose what axis to choose for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1 : given an array, we are adding up the absolute value of its element, and the resulting number is the l1 norm.\n",
    "\n",
    "---example : $$\\text{L1} ([1,2,3,4]) = |1| + |2| + |3| + |4| = 10$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 : given an array, we are adding up the square of its element, and the square root of the resulting number is        the l2 norm.\n",
    "\n",
    "-- example : $$\\text{L2} ([1,2,3,4])= \\sqrt{1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{30}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 : given an array, we are taking the maximum value of the absolute values of its elements\n",
    "\n",
    "-- example : $$\\text{maxnorm} ([1,2,3,-4])= max(1|,|2|,|3|,|-4|) = 4$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizations:\n",
    "    ''' \n",
    "    l1/l2/maxnorm normalizations. \n",
    "    Be aware, it doesn't catch zero division. you can improve it to catch zero vectors in the re_shape method helper.\n",
    "    ''' \n",
    "    def __init__(self,matrix):\n",
    "        self.matrix = np.array(matrix)        \n",
    "    def re_shape(self,norm,axis):\n",
    "        '''\n",
    "        reshapes the norm to fit the matrix\n",
    "        '''\n",
    "        if axis ==1:\n",
    "            resultant = np.repeat(norm,self.matrix.shape[axis]).reshape(self.matrix.shape)\n",
    "        elif axis==0:\n",
    "            resultant = np.repeat(norm,self.matrix.shape[axis]).reshape(self.matrix.shape[::-1]).T\n",
    "        return resultant\n",
    "    def l1_normalization(self,axis=1):\n",
    "        '''\n",
    "        returns l1-normalized matrix\n",
    "        '''\n",
    "        l1_values = np.sum(abs(self.matrix),axis)\n",
    "        resultant = self.re_shape(l1_values,axis)\n",
    "        l1 = self.matrix / resultant\n",
    "        return l1\n",
    "    def l2_normalization(self,axis =1):\n",
    "        '''\n",
    "        returns l2-normalized matrix\n",
    "        '''\n",
    "        l2_values = np.sqrt(np.sum(self.matrix**2,axis))\n",
    "        resultant = self.re_shape(l2_values,axis)\n",
    "        l2 = self.matrix / resultant\n",
    "        return l2\n",
    "    def maxnorm_normalization(self,axis =1):\n",
    "        '''\n",
    "        returns maxnorm-normalized matrix\n",
    "        '''\n",
    "        maxnorm_values = abs(self.matrix).max(axis)\n",
    "        resultant = self.re_shape(maxnorm_values,axis)\n",
    "        maxnorm = self.matrix / resultant\n",
    "        return maxnorm\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency and Inverse Document Frequency :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{raw term frequency tf(t,d)---number of time a term t occurs in a document d} $\n",
    "$ \\text{Inverse document frequency idf(t,d)---number of documents d that contains the term t} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{ tf(t,d) = } \\frac{n_t}{n_w}  \\text{ ,with :} \\bigg\\{_{n_w \\text{the total number of terms in the document}}^{n_t \\text{count of t appears in a document}} $  \n",
    "$ \\text{idf(t,d) = } log_e(\\frac{n_d}{n_{dt}})+1 \\text{ ,with :} \\bigg\\{^{n_d \\text{the total number of documents}}_{n_{dt} \\text{the number of documents with term t in it}}$\n",
    "$ \\text{We are using the logarithm to ensure that low document frequencies are not given too much weight.} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\text{ tfidf(t,d) = tf(t,d) * idf(t,d) }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf:\n",
    "    def __init__(self,matrix,axis = 1):\n",
    "        self.matrix = np.array(matrix)\n",
    "        self.axis = axis\n",
    "\n",
    "    def _Norm(self,tf,norm):\n",
    "        '''\n",
    "        Normalize the matrix with the chosen norm\n",
    "        '''\n",
    "        if norm == 'l2':\n",
    "            tf = Normalizations(tf).l2_normalization()\n",
    "        elif norm == 'l1':\n",
    "            tf = Normalizations(tf).l1_normalization()\n",
    "        elif norm == 'maxnorm':\n",
    "            tf = Normalizations(tf).maxnorm_normalization()\n",
    "        return tf\n",
    "    def tfidf(self,norm = 'l2',idf = True,smooth = True):\n",
    "        '''\n",
    "        return matrix adjusted with tfidf\n",
    "        '''\n",
    "        tf = self.tf(norm,idf,smooth)\n",
    "        if idf:\n",
    "            tfidf = tf * self.idf(smooth)\n",
    "        else:\n",
    "            tfidf = tf\n",
    "        tfidf = self._Norm(tfidf,norm)\n",
    "        return tfidf\n",
    "    def tf(self,norm = 'l2',idf = True,smooth = True):\n",
    "        '''\n",
    "        returns tf\n",
    "        '''\n",
    "        nw = self.matrix.shape[self.axis] #total number of terms in doc\n",
    "        resultant = np.ones(self.matrix.shape) * nw\n",
    "        tf = self.matrix / nw\n",
    "        tf = self._Norm(tf,norm)\n",
    "        return tf \n",
    "    \n",
    "    def idf(self,smooth):\n",
    "        '''\n",
    "        returns idf\n",
    "        '''\n",
    "        nd = self.matrix.shape[1-self.axis] #total number of documents\n",
    "        ndt = nd - np.sum(self.matrix == 0 ,1-self.axis) # number of documents with term t\n",
    "        idf = np.log((nd)/(ndt)) +1 \n",
    "        if smooth: # prevents zero divisions\n",
    "            idf = np.log((nd+1)/(ndt+1)) +1 # we add on            \n",
    "        if self.axis == 0:\n",
    "            idf= np.repeat(idf,self.matrix.shape[1]).reshape(self.matrix.shape)\n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.  , 0.56, 0.56, 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.  , 0.56, 0.43, 0.  , 0.56],\n",
       "       [0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf_idf(bag.to_array(),1)\n",
    "test.tfidf(idf = True,norm = 'l2',smooth = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Let's do some data cleaning : \n",
    "##### preprocessing the reviews to remove html tags,  and moving emoticons to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  hello       haha   :) :p :/'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(txt):\n",
    "    '''\n",
    "    light preprocessing of the text\n",
    "    '''\n",
    "    txt+= ' ' # adding space for the emogies not to stick to last word\n",
    "    # removing html tags\n",
    "    txt = re.sub('<[^>]*>',' ', txt) \n",
    "\n",
    "    # puting emoticons to the end and puting everything to lowercase\n",
    "    em_reg = '(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)'\n",
    "    emoticons = re.findall(em_reg,txt)\n",
    "    txt = re.sub(em_reg ,' ', txt.lower() ) + ' '.join(emoticons)\n",
    "    return txt\n",
    "# let's test it\n",
    "preprocessor('<p> hello :) :p :/ haha </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "# to improve the tokenizer, we will also be removing stopwords such as I,me...\n",
    "nltk.download('stopwords') # downloading stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenizer_porter(text,stop = None):\n",
    "    if type(stop) == type(None):\n",
    "        return [porter.stem(word) for word in text.split()] \n",
    "    else :\n",
    "        return [porter.stem(word) for word in text.split() if word not in stop] \n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words :  ['I', 'know', 'swimmer', 'swim', 'swim', 'pool', 'full', 'swimmer']\n",
      "Not removing stop words :  ['I', 'know', 'a', 'swimmer', 'who', 'is', 'swim', 'in', 'a', 'swim', 'pool', 'full', 'of', 'swimmer']\n"
     ]
    }
   ],
   "source": [
    "# let's test it\n",
    "test = 'I know a swimmer who is swimming in a swimming pool full of swimmers'\n",
    "print('Removing stop words : ' , tokenizer_porter(test,stop))\n",
    "print('Not removing stop words : ' , tokenizer_porter(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>anoth outstand foreign film thoroughli trounc ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10230</th>\n",
       "      <td>young cat tri steal back brother soul death ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19387</th>\n",
       "      <td>movie' storylin pat quaint. two women travel m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews sentiment\n",
       "5822   anoth outstand foreign film thoroughli trounc ...         1\n",
       "10230  young cat tri steal back brother soul death ge...         1\n",
       "19387  movie' storylin pat quaint. two women travel m...         0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the preprocessing on all of the reviews\n",
    "def preprocessing(df,col = 'reviews'):\n",
    "    df[col] = df[col].apply(preprocessor)\n",
    "    df[col] = df[col].apply(lambda x : ' '.join(tokenizer_porter(x,stop)) ) #parsing reviews\n",
    "preprocessing(reviews_train)    \n",
    "reviews_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 out of 25000\n",
      "2.0%\n"
     ]
    }
   ],
   "source": [
    "# let's put everything together\n",
    "# the process below is my understanding of how tfidfVectorizer works\n",
    "word2vec = Word2Vec() # defining the word2vec\n",
    "rev_num = 0\n",
    "rev_tot = reviews_train.shape[0]\n",
    "X_mine = []\n",
    "\n",
    "iteration = 2 # I only use few iterations to compare it with tfidfvectorizer because the computation is \n",
    "#heavy \n",
    "# first we have to preprocess then tokenize the reviews which has already been done above\n",
    "#second we have to add the vocabulary to Word2vec\n",
    "for review in reviews_train.reviews[:iteration]:\n",
    "    rev_num +=1\n",
    "    clear_output(wait = True)\n",
    "    word2vec.fit([review]) # fitting word2vec with the parsed review\n",
    "    print(f'{rev_num} out of {rev_tot}')\n",
    "    print(f'{(rev_num/rev_tot) * 100}%')\n",
    "#third we apply tfidf\n",
    "rev_num = 0\n",
    "for review in reviews_train.reviews[:iteration]:\n",
    "    clear_output(wait = True)\n",
    "    rev_num +=1\n",
    "    tfidf_mine = tf_idf(word2vec.to_array(arr = [review]))\n",
    "    X_mine.append(tfidf_mine.tfidf(idf = True,norm = 'l2',smooth = True) )\n",
    "    print(f'{rev_num} out of {rev_tot}')\n",
    "    print(f'{(rev_num/rev_tot) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents = False, lowercase = False, preprocessor = None,\n",
    "                        tokenizer = tokenizer_porter, use_idf = True, norm = 'l2',\n",
    "                       smooth_idf = True) \n",
    "\n",
    "y = reviews_train.sentiment.values.astype('int')\n",
    "X_lib = tfidf.fit_transform(reviews_train.reviews[:iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary not in my word2vec : ['(certainli', '(ie.th', 'acid,', 'back,', 'bathhouse,', 'dali.', 'disappointed.', 'disturbing,', 'eat).thi', 'forever.', 'half...', 'hollywood!', \"i'm\", 'it.you', 'life.', 'lives,', 'living.', 'magnificent,', 'movie.', 'never-end', 'no,', 'not.', 'ourselves.th', 'owner,', 'provoking.sit', 'rather,', 'relax,', 'strange,', 'work).if']\n",
      "nbr of vocabulary not in my word2vec not taking into account punctuation: []\n"
     ]
    }
   ],
   "source": [
    "# let's compare my Word2vec and the tfidfVectorizer in terms of result (not performance and speed)\n",
    "different = []\n",
    "for voc in tfidf.get_feature_names():\n",
    "    if voc not in word2vec.vocabulary().keys():\n",
    "        different.append(voc)\n",
    "print (f'vocabulary not in my word2vec : {different}') # those words are being removed from my word2vec because I\n",
    "# defined a purge_alph method that gets rid all non-alpha characters in the words and removes the duplicates, \n",
    "#so technically those words are in my word2vec vocabulary, but without the punctuation\n",
    "# Proof :\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "new_different = []\n",
    "for word in different:\n",
    "    new_words = regex.sub(' ', word).split()\n",
    "    for new_word in new_words:\n",
    "        if new_word not in word2vec.vocabulary().keys():\n",
    "            new_different.append(new_word)\n",
    "        \n",
    "print (f'nbr of vocabulary not in my word2vec not taking into account punctuation: { new_different }') \n",
    "# you will usually find weird looking words that are basically that were tokenized differently by Stemmer (due to\n",
    "# the fact that one had a special character and the other one didn't)\n",
    "\n",
    "# the tf-idf resulting matrices will be a bit different because the vocabularies are not the same and the order is\n",
    "# different too, but they both work well, but the speed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I will obviously not be making it from scratch everytime I want to use it, but doing once helped me learn a great deal about how some of the features of CountVectorizer, TfidfTransformer, and TfidfVectorizer work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's use the TfidfVectorizer for the rest of this Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents = False, lowercase = False, preprocessor = None,# already applied the prepro\n",
    "                        tokenizer = tokenizer_porter, use_idf = True, norm = 'l2', # and lowered the characters\n",
    "                       smooth_idf = True) \n",
    "\n",
    "y = reviews_train.sentiment.values.astype('int')\n",
    "X = tfidf.fit_transform(reviews_train.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "dico = tfidf.vocabulary_\n",
    "for i in dico:\n",
    "    dico[i] = int(dico[i]) # converting numpy int to int for json to recognize them\n",
    "with open('vocabulary.json','w') as fp:\n",
    "    json.dump(dico,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.datasets.dump_svmlight_file(X,y,f = 'training_x_y.feat') # saving X and y as libsvm sparse matrixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X,y = sklearn.datasets.load_svmlight_file('training_x_y.feat') # loading them\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed: 37.7min remaining: 56.6min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 37.9min remaining: 25.3min\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 43.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 43.0min finished\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5,\n",
    "                          scoring = 'accuracy',\n",
    "                          random_state = 7,\n",
    "                          n_jobs = -1, # using all the processors \n",
    "                          verbose = 10,\n",
    "                          max_iter = 300)\n",
    "clf.fit(X,y)\n",
    "filename = 'LR_model.sav'\n",
    "check_ = open(filename,'wb')\n",
    "pickle.dump(clf,check_)\n",
    "check_.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# loading the saved model\n",
    "filename = 'LR_model.sav'\n",
    "clf = pickle.load(open(filename,'rb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the test data\n",
    "positive_dir_test = './test/pos/'\n",
    "positive_reviews = review_extraction(positive_dir_test)\n",
    "\n",
    "negative_dir_test = './test/neg/'\n",
    "negative_reviews_test = review_extraction(negative_dir_test)\n",
    "\n",
    "reviews0_test = pd.DataFrame(np.c_[negative_reviews,np.zeros(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews1_test = pd.DataFrame(np.c_[positive_reviews,np.ones(len(negative_reviews),dtype = 'uint8')],\n",
    "                        columns = ['reviews','sentiment'])\n",
    "\n",
    "reviews_test = reviews1_test.append(reviews0_test,ignore_index = True)\n",
    "preprocessing(reviews_test)    \n",
    "\n",
    "y_test = reviews_test.sentiment.values.astype('int')\n",
    "X_test = tfidf.transform(reviews_test.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.93 \n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "print(f'accuracy : {round(clf.score(X_test,y_test),2)} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n"
     ]
    }
   ],
   "source": [
    "# write whatever comment you want and see whether the machine thinks it's a good or bad comment\n",
    "# write your comment in text_test\n",
    "text_test = \"I really liked the film, especially when the hero saved the girl,\\\n",
    "            for the rest the scenario was banal.\"\n",
    "text_test = preprocessor ( text_test )\n",
    "text_test = tokenizer_porter(text_test,stop)\n",
    "\n",
    "xx_test = tfidf.transform(text_test)\n",
    "prediction = np.mean(clf.predict(xx_test))\n",
    "if prediction <0.5:\n",
    "    print('bad review')\n",
    "elif prediction>.5:\n",
    "    print('good review')\n",
    "else:\n",
    "    print('neutral')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
